{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6119,"status":"ok","timestamp":1697035503041,"user":{"displayName":"potato kaggle","userId":"05409133564111543032"},"user_tz":-540},"id":"R-t51mhwVtzL","outputId":"abc80dea-a48f-4c5a-f72e-e1e2e42ecf6e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.16)\n","Requirement already satisfied: six\u003e=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2023.7.22)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.1)\n","Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.1)\n","Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.6)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.0.0)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach-\u003ekaggle) (0.5.1)\n","Requirement already satisfied: text-unidecode\u003e=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify-\u003ekaggle) (1.3)\n","Requirement already satisfied: charset-normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.10/dist-packages (from requests-\u003ekaggle) (3.3.0)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-\u003ekaggle) (3.4)\n"]}],"source":["!pip install kaggle"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":44098,"status":"ok","timestamp":1697035547133,"user":{"displayName":"potato kaggle","userId":"05409133564111543032"},"user_tz":-540},"id":"77ca2bFIc9rw","outputId":"842e8f4b-65ba-417d-e94b-96331bd8a894"},"outputs":[{"name":"stdout","output_type":"stream","text":["  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m950.8/950.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for keras-cv (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["! pip install -q git+https://github.com/keras-team/keras-cv"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IlJKtstYV_94"},"outputs":[],"source":["# mount the drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yQfATWvTWRDR"},"outputs":[],"source":["import os\n","# You can use `tensorflow`, `pytorch`, `jax` here\n","# KerasCore makes the notebook backend agnostic :)\n","os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n","\n","import keras_cv\n","import keras_core as keras\n","from keras_core import layers\n","\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from matplotlib import pyplot as plt\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"markdown","metadata":{"id":"23qYpUFeXnR-"},"source":["Config"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"quybXx-6XvRJ"},"outputs":[],"source":["class Config:\n","    SEED = 42\n","    IMAGE_SIZE = [256, 256]\n","    BATCH_SIZE = 16\n","    EPOCHS = 15\n","    TARGET_COLS = [\n","        \"bowel\"\n","    ]\n","    AUTOTUNE = tf.data.AUTOTUNE\n","\n","config = Config()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ezZWvE3xW6lm"},"outputs":[],"source":["keras.utils.set_random_seed(seed=config.SEED)"]},{"cell_type":"markdown","metadata":{"id":"bDfeF7AYXyaa"},"source":["Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RsWnShauXrN5"},"outputs":[],"source":["BASE_PATH = \"/content/drive/MyDrive/rsna_data\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xLn5bi3fX5tP"},"outputs":[],"source":["# CSV 파일을 읽어와서 데이터프레임 생성\n","train_df = pd.read_csv(f\"{BASE_PATH}/train.csv\")\n","series_meta_df = pd.read_csv(f\"{BASE_PATH}/train_series_meta.csv\")\n","\n","# train.csv와 train_series_meta.csv를 patient_id를 기준으로 병합\n","dataframe = pd.merge(train_df, series_meta_df, on=\"patient_id\")\n","\n","# 이미지 경로 생성\n","dataframe[\"image_path\"] = f\"/content/drive/MyDrive/png_jjw\"\\\n","                    + \"/\" + dataframe.patient_id.astype(str)\\\n","                    + \"/\" + dataframe.series_id.astype(str)\\\n","                    + \"/\" + \"img_256x256_d1_original\"\\\n","\n","# 처음 2개 행 출력\n","dataframe.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JKKwx4lEFmCf"},"outputs":[],"source":["#dataframe = dataframe[dataframe['any_injury'] == 1]\n","print(dataframe.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8OwXLeR1qEfH"},"outputs":[],"source":["def bowel_assign_value(row):\n","    if row['bowel_healthy'] == 1:\n","        return 0\n","    else:\n","        return 1\n","\n","def extra_assign_value(row):\n","    if row['extravasation_healthy'] == 1:\n","        return 0\n","    else:\n","        return 1\n","\n","def kid_assign_value(row):\n","    if row['kidney_healthy'] == 1:\n","        return 0\n","    elif row['kidney_low'] == 1:\n","        return 1\n","    elif row['kidney_high'] == 1:\n","        return 2\n","    else:\n","        return None\n","def liver_assign_value(row):\n","    if row['liver_healthy'] == 1:\n","        return 0\n","    elif row['liver_low'] == 1:\n","        return 1\n","    elif row['liver_high'] == 1:\n","        return 2\n","    else:\n","        return None\n","\n","def spleen_assign_value(row):\n","    if row['spleen_healthy'] == 1:\n","        return 0\n","    elif row['spleen_low'] == 1:\n","        return 1\n","    elif row['spleen_high'] == 1:\n","        return 2\n","    else:\n","        return None\n","\n","dataframe['bowel'] = dataframe.apply(bowel_assign_value, axis=1)\n","dataframe['extravasation'] = dataframe.apply(extra_assign_value, axis=1)\n","dataframe['kidney'] = dataframe.apply(kid_assign_value, axis=1)\n","dataframe['liver'] = dataframe.apply(liver_assign_value, axis=1)\n","dataframe['spleen'] = dataframe.apply(spleen_assign_value, axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TQHIMrkKunPR"},"outputs":[],"source":["dataframe[[\"patient_id\", \"bowel\", \"extravasation\", \"kidney\", \"liver\", \"spleen\", \"image_path\"]]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S4TJs8edeLHw"},"outputs":[],"source":["negative = dataframe[dataframe['bowel'] == 0]\n","positive = dataframe[dataframe['bowel'] == 1]\n","num_samples = min(len(negative), len(positive))\n","negative_samples = negative.sample(n=num_samples, random_state=42)\n","positive_samples = positive.sample(n=num_samples, random_state=42)\n","bowel_dataframe = pd.concat([negative_samples, positive_samples], axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SmpK-8cdfHbx"},"outputs":[],"source":["negative = dataframe[dataframe['extravasation'] == 0]\n","positive = dataframe[dataframe['extravasation'] == 1]\n","num_samples = min(len(negative), len(positive))\n","negative_samples = negative.sample(n=num_samples, random_state=42)\n","positive_samples = positive.sample(n=num_samples, random_state=42)\n","extra_dataframe = pd.concat([negative_samples, positive_samples], axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IgDUx7OJfg3X"},"outputs":[],"source":["\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aFr0mn9Igdfq"},"outputs":[],"source":["negative = dataframe[dataframe['kidney'] == 0]\n","positive1 = dataframe[dataframe['kidney'] == 1]\n","positive2 = dataframe[dataframe['kidney'] == 2]\n","num_samples = min(len(negative), len(positive1), len(positive2))\n","negative_samples = negative.sample(n=num_samples, random_state=42)\n","positive1_samples = positive1.sample(n=num_samples, random_state=42)\n","positive2_samples = positive2.sample(n=num_samples, random_state=42)\n","kidney_dataframe = pd.concat([negative_samples, positive1_samples, positive2_samples], axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FVwXxmdLgoN0"},"outputs":[],"source":["negative = dataframe[dataframe['spleen'] == 0]\n","positive1 = dataframe[dataframe['spleen'] == 1]\n","positive2 = dataframe[dataframe['spleen'] == 2]\n","num_samples = min(len(negative), len(positive1), len(positive2))\n","negative_samples = negative.sample(n=num_samples, random_state=42)\n","positive1_samples = positive1.sample(n=num_samples, random_state=42)\n","positive2_samples = positive2.sample(n=num_samples, random_state=42)\n","spleen_dataframe = pd.concat([negative_samples, positive1_samples, positive2_samples], axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wMnU7GL00H6U"},"outputs":[],"source":["spleen_dataframe.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nu9XchDNX-iv"},"outputs":[],"source":["# Function to handle the split for each group\n","def split_group(group, test_size=0.2):\n","    if len(group) == 1:\n","        return (group, pd.DataFrame()) if np.random.rand() \u003c test_size else (pd.DataFrame(), group)\n","    else:\n","        return train_test_split(group, stratify=group[\"bowel\"], test_size=test_size, random_state=42)\n","\n","# Initialize the train and validation datasets\n","bowel_train_data = pd.DataFrame()\n","bowel_val_data = pd.DataFrame()\n","extra_train_data = pd.DataFrame()\n","extra_val_data = pd.DataFrame()\n","liver_train_data = pd.DataFrame()\n","liver_val_data = pd.DataFrame()\n","kidney_train_data = pd.DataFrame()\n","kidney_val_data = pd.DataFrame()\n","spleen_train_data = pd.DataFrame()\n","spleen_val_data = pd.DataFrame()\n","\n","# Iterate through the groups and split them, handling single-sample groups\n","for _, group in bowel_dataframe.groupby(config.TARGET_COLS):\n","    bowel_train_group, bowel_val_group = split_group(group)\n","    bowel_train_data = pd.concat([bowel_train_data, bowel_train_group], ignore_index=True)\n","    bowel_val_data = pd.concat([bowel_val_data, bowel_val_group], ignore_index=True)\n","\n","for _, group in extra_dataframe.groupby(config.TARGET_COLS):\n","    extra_train_group, extra_val_group = split_group(group)\n","    extra_train_data = pd.concat([extra_train_data, extra_train_group], ignore_index=True)\n","    extra_val_data = pd.concat([extra_val_data, extra_val_group], ignore_index=True)\n","\n","for _, group in liver_dataframe.groupby(config.TARGET_COLS):\n","    liver_train_group, liver_val_group = split_group(group)\n","    liver_train_data = pd.concat([liver_train_data, liver_train_group], ignore_index=True)\n","    liver_val_data = pd.concat([liver_val_data, liver_val_group], ignore_index=True)\n","\n","for _, group in kidney_dataframe.groupby(config.TARGET_COLS):\n","    kidney_train_group, kidney_val_group = split_group(group)\n","    kidney_train_data = pd.concat([kidney_train_data, kidney_train_group], ignore_index=True)\n","    kidney_val_data = pd.concat([kidney_val_data, kidney_val_group], ignore_index=True)\n","\n","for _, group in spleen_dataframe.groupby(config.TARGET_COLS):\n","    spleen_train_group, spleen_val_group = split_group(group)\n","    spleen_train_data = pd.concat([spleen_train_data, spleen_train_group], ignore_index=True)\n","    spleen_val_data = pd.concat([spleen_val_data, spleen_val_group], ignore_index=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jtbt681pYCLo"},"outputs":[],"source":["bowel_train_data.shape, bowel_val_data.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JAB0B4v3GBzH"},"outputs":[],"source":["# print(train_data['bowel_injury'].value_counts())\n","# print(train_data['extravasation_injury'].value_counts())\n","# print(train_data['liver_high'].value_counts())\n","# print(train_data['liver_low'].value_counts())\n","# print(train_data['kidney_high'].value_counts())\n","# print(train_data['kidney_low'].value_counts())\n","# print(train_data['spleen_high'].value_counts())\n","# print(train_data['spleen_low'].value_counts())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qn94sbxWGi2s"},"outputs":[],"source":["# print(val_data['bowel_injury'].value_counts())\n","# print(val_data['extravasation_injury'].value_counts())\n","# print(val_data['liver_high'].value_counts())\n","# print(val_data['liver_low'].value_counts())\n","# print(val_data['kidney_high'].value_counts())\n","# print(val_data['kidney_low'].value_counts())\n","# print(val_data['spleen_high'].value_counts())\n","# print(val_data['spleen_low'].value_counts())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lnGHaN1CYD0f"},"outputs":[],"source":["def decode_image_and_label(image_path, label):\n","    file_bytes1 = tf.io.read_file(image_path+'/image_001.png')\n","    image1 = tf.io.decode_png(file_bytes1, channels=1, dtype=tf.uint8)\n","    file_bytes2 = tf.io.read_file(image_path+'/image_002.png')\n","    image2 = tf.io.decode_png(file_bytes2, channels=1, dtype=tf.uint8)\n","    file_bytes3 = tf.io.read_file(image_path+'/image_003.png')\n","    image3 = tf.io.decode_png(file_bytes3, channels=1, dtype=tf.uint8)\n","    image = tf.concat([image1, image2, image3], axis=2)\n","\n","    image = tf.image.resize(image, config.IMAGE_SIZE, method=\"bilinear\")\n","    image = tf.cast(image, tf.float32) / 255.0\n","\n","    label = tf.cast(label, tf.float32)\n","\n","    return (image, label)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eAL7PxEGYFaS"},"outputs":[],"source":["# 레이어 외부에서 RandomFlip 레이어를 생성\n","random_flip_layer = tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal\")\n","random_rotation_layer = tf.keras.layers.experimental.preprocessing.RandomRotation(0.2)\n","\n","class CustomAugmenter(tf.keras.layers.Layer):\n","    def __init__(self, cutout_params, **kwargs):\n","        super(CustomAugmenter, self).__init__(**kwargs)\n","        self.cutout_layer = keras_cv.layers.Augmenter([keras_cv.layers.RandomCutout(**cutout_params)])\n","\n","    def call(self, inputs, training=None):\n","        if training:\n","            inputs = random_flip_layer(inputs)\n","            inputs = random_rotation_layer(inputs)\n","            inputs = self.cutout_layer(inputs)\n","        return inputs\n","\n","def apply_augmentation(images, labels):\n","    # 이미지 증강 파이프라인을 정의\n","    augmenter = CustomAugmenter(cutout_params={\"height_factor\": 0.2, \"width_factor\": 0.2})\n","\n","    # 이미지 증강을 적용\n","    augmented_images = augmenter(images, training=True)\n","\n","    return (augmented_images, labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kaybtZl8YG9s"},"outputs":[],"source":["def build_dataset(image_paths, labels):\n","    ds = (\n","        tf.data.Dataset.from_tensor_slices((image_paths, labels))\n","        .map(decode_image_and_label, num_parallel_calls=config.AUTOTUNE)\n","        .shuffle(config.BATCH_SIZE * 10)\n","        .batch(config.BATCH_SIZE)\n","        .map(apply_augmentation, num_parallel_calls=config.AUTOTUNE)  # 이미지 증강 적용\n","        .prefetch(config.AUTOTUNE)\n","    )\n","    return ds"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tXK899mA71Nr"},"outputs":[],"source":["paths = bowel_train_data.image_path.tolist()\n","labels = bowel_train_data[config.TARGET_COLS].values\n","print(len(paths))\n","print(labels.shape)"]},{"cell_type":"markdown","metadata":{"id":"r35Z8p5nue1l"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HGjkekmCYI71"},"outputs":[],"source":["paths = bowel_train_data.image_path.tolist()\n","labels = bowel_train_data[config.TARGET_COLS].values\n","\n","ds = build_dataset(image_paths=paths, labels=labels)\n","images, labels = next(iter(ds))\n","images.shape, [label.shape for label in labels]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GUY-pZMPYK3z"},"outputs":[],"source":["keras_cv.visualization.plot_image_gallery(\n","    images=images,\n","    value_range=(0, 1),\n","    rows=2,\n","    cols=2,\n",")"]},{"cell_type":"markdown","metadata":{"id":"rmcvaMvtbhyE"},"source":["Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nMBVJMi79XYC"},"outputs":[],"source":["import tensorflow as tf\n","from sklearn.metrics import confusion_matrix\n","\n","# Custom metric to calculate sensitivity\n","def sensitivity(y_true, y_pred):\n","    true_positives = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(y_true, 1), tf.equal(tf.round(y_pred), 1)), dtype=tf.float32))\n","    actual_positives = tf.reduce_sum(tf.cast(tf.equal(y_true, 1), dtype=tf.float32))\n","    return true_positives / (actual_positives + tf.keras.backend.epsilon())\n","\n","# Custom metric to calculate specificity\n","def specificity(y_true, y_pred):\n","    true_negatives = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(y_true, 0), tf.equal(tf.round(y_pred), 0)), dtype=tf.float32))\n","    actual_negatives = tf.reduce_sum(tf.cast(tf.equal(y_true, 0), dtype=tf.float32))\n","    return true_negatives / (actual_negatives + tf.keras.backend.epsilon())"]},{"cell_type":"markdown","metadata":{"id":"Ux1M9hViexZM"},"source":["Efficientnet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uQ5TczQBbTTa"},"outputs":[],"source":["def build_binary_classification_model(warmup_steps, decay_steps, head_name):\n","    # Define Input\n","    inputs = keras.Input(shape=config.IMAGE_SIZE + [3,], batch_size=config.BATCH_SIZE)\n","\n","    # Define Backbone\n","    backbone = keras_cv.models.EfficientNetV2Backbone.from_preset(\"efficientnetv2_b3\")\n","    backbone.include_rescaling = False\n","    x = backbone(inputs)\n","\n","    # GAP to get the activation maps\n","    gap = keras.layers.GlobalAveragePooling2D()\n","    x = gap(x)\n","\n","    # Define 'necks' for the binary classification head\n","    x_head = keras.layers.Dense(32, activation='silu')(x)\n","\n","    # Define binary classification head\n","    output = keras.layers.Dense(1, name=head_name, activation='sigmoid')(x_head)\n","\n","    # Create model\n","    print(f\"[INFO] Building the {head_name} model...\")\n","    model = keras.Model(inputs=inputs, outputs=output)\n","\n","    # Cosine Decay\n","    cosine_decay = keras.optimizers.schedules.CosineDecay(\n","        initial_learning_rate=1e-4,\n","        decay_steps=decay_steps,\n","        alpha=0.0,\n","        warmup_target=1e-3,\n","        warmup_steps=warmup_steps,\n","    )\n","\n","    # Compile the model\n","    optimizer = keras.optimizers.Adam(learning_rate=cosine_decay)\n","    loss = keras.losses.BinaryCrossentropy()\n","    metrics = [\"accuracy\", sensitivity, specificity]\n","\n","    print(f\"[INFO] Compiling the {head_name} model...\")\n","    model.compile(\n","        optimizer=optimizer,\n","        loss=loss,\n","        metrics=metrics\n","    )\n","\n","    return model\n","\n","def build_tertiary_classification_model(warmup_steps, decay_steps, head_name):\n","    # Define Input\n","    inputs = keras.Input(shape=config.IMAGE_SIZE + [3,], batch_size=config.BATCH_SIZE)\n","\n","    # Define Backbone\n","    backbone = keras_cv.models.EfficientNetV2Backbone.from_preset(\"efficientnetv2_b3\")\n","    backbone.include_rescaling = False\n","    x = backbone(inputs)\n","\n","    # GAP to get the activation maps\n","    gap = keras.layers.GlobalAveragePooling2D()\n","    x = gap(x)\n","\n","    # Define 'necks' for the tertiary classification head\n","    x_head = keras.layers.Dense(32, activation='silu')(x)\n","\n","    # Define tertiary classification head\n","    output = keras.layers.Dense(3, name=head_name, activation='softmax')(x_head)\n","\n","    # Create model\n","    print(f\"[INFO] Building the {head_name} model...\")\n","    model = keras.Model(inputs=inputs, outputs=output)\n","\n","    # Cosine Decay\n","    cosine_decay = keras.optimizers.schedules.CosineDecay(\n","        initial_learning_rate=1e-4,\n","        decay_steps=decay_steps,\n","        alpha=0.0,\n","        warmup_target=1e-3,\n","        warmup_steps=warmup_steps,\n","    )\n","\n","    # Compile the model\n","    optimizer = keras.optimizers.Adam(learning_rate=cosine_decay)\n","    loss = keras.losses.CategoricalCrossentropy()\n","    metrics = [\"accuracy\"]\n","\n","    print(f\"[INFO] Compiling the {head_name} model...\")\n","    model.compile(\n","        optimizer=optimizer,\n","        loss=loss,\n","        metrics=metrics\n","    )\n","\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"QHOb-3zPbmF0"},"source":["Train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0S8xxkyybkjD"},"outputs":[],"source":["# get image_paths and labels\n","print(\"[INFO] Building the dataset...\")\n","train_paths = bowel_train_data.image_path.values; train_labels = bowel_train_data[config.TARGET_COLS].values.astype(np.float32)\n","valid_paths = bowel_val_data.image_path.values; valid_labels = bowel_val_data[config.TARGET_COLS].values.astype(np.float32)\n","\n","# train and valid dataset\n","train_ds = build_dataset(image_paths=train_paths, labels=train_labels)\n","val_ds = build_dataset(image_paths=valid_paths, labels=valid_labels)\n","\n","total_train_steps = train_ds.cardinality().numpy() * config.BATCH_SIZE * config.EPOCHS\n","warmup_steps = int(total_train_steps * 0.10)\n","decay_steps = total_train_steps - warmup_steps\n","\n","print(f\"{total_train_steps=}\")\n","print(f\"{warmup_steps=}\")\n","print(f\"{decay_steps=}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"9FP3oSNAbmwV"},"outputs":[{"name":"stdout","output_type":"stream","text":["[INFO] Building the bowel model...\n","[INFO] Compiling the bowel model...\n","Epoch 1/15\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1897s\u001b[0m 127s/step - accuracy: 0.4070 - loss: 0.7203 - mean_metric_wrapper: 0.4154 - mean_metric_wrapper_1: 0.4005 - val_accuracy: 0.5000 - val_loss: 0.7229 - val_mean_metric_wrapper: 0.0000e+00 - val_mean_metric_wrapper_1: 1.0000\n","Epoch 2/15\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m767s\u001b[0m 69s/step - accuracy: 0.5135 - loss: 0.7205 - mean_metric_wrapper: 0.5106 - mean_metric_wrapper_1: 0.5707 - val_accuracy: 0.5000 - val_loss: 0.6931 - val_mean_metric_wrapper: 1.0000 - val_mean_metric_wrapper_1: 0.0000e+00\n","Epoch 3/15\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m763s\u001b[0m 66s/step - accuracy: 0.5926 - loss: 0.6990 - mean_metric_wrapper: 0.6845 - mean_metric_wrapper_1: 0.5349 - val_accuracy: 0.5000 - val_loss: 0.6929 - val_mean_metric_wrapper: 0.0000e+00 - val_mean_metric_wrapper_1: 1.0000\n","Epoch 4/15\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m774s\u001b[0m 69s/step - accuracy: 0.6451 - loss: 0.6705 - mean_metric_wrapper: 0.5946 - mean_metric_wrapper_1: 0.5830 - val_accuracy: 0.5000 - val_loss: 0.6958 - val_mean_metric_wrapper: 1.0000 - val_mean_metric_wrapper_1: 0.0000e+00\n","Epoch 5/15\n","\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m726s\u001b[0m 66s/step - accuracy: 0.6206 - loss: 0.6852 - mean_metric_wrapper: 0.4675 - mean_metric_wrapper_1: 0.7746 - val_accuracy: 0.5000 - val_loss: 0.6974 - val_mean_metric_wrapper: 1.0000 - val_mean_metric_wrapper_1: 0.0000e+00\n","Epoch 6/15\n","\u001b[1m 6/11\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m5:22\u001b[0m 65s/step - accuracy: 0.5099 - loss: 0.7191 - mean_metric_wrapper: 0.4147 - mean_metric_wrapper_1: 0.6038"]}],"source":["# Directory where you want to save the models\n","save_dir = BASE_PATH + \"/checkpoint/\"\n","\n","# List of model names\n","model_names = [\"bowel\"]\n","\n","# Create a 1x2 grid for the subplots\n","fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n","\n","# Flatten axes to iterate through them\n","axes = axes.flatten()\n","\n","for i, name in enumerate(model_names):\n","    # Build the model\n","    if name in [\"bowel\", \"extra\"]:\n","        model = build_binary_classification_model(warmup_steps, decay_steps, name)\n","    else:\n","        model = build_tertiary_classification_model(warmup_steps, decay_steps, name)\n","\n","    # Train the model\n","    history = model.fit(train_ds, epochs=config.EPOCHS, validation_data=val_ds)\n","\n","    model_filename = f\"EfficinetnetB3_{name}.keras\"\n","    model_path = os.path.join(save_dir, model_filename)\n","    model.save(model_path)\n","\n","    # Plot training accuracy\n","    axes[0].plot(history.history['accuracy'], label='Training ' + name)\n","    # Plot validation accuracy\n","    axes[1].plot(history.history['val_accuracy'], label='Validation ' + name)\n","\n","    axes[0].set_title(\"Training Accuracy\")\n","    axes[1].set_title(\"Validation Accuracy\")\n","    axes[0].set_xlabel('Epoch')\n","    axes[1].set_xlabel('Epoch')\n","    axes[0].set_ylabel('Accuracy')\n","    axes[1].set_ylabel('Accuracy')\n","    axes[0].legend()\n","    axes[1].legend()\n","\n","    plt.tight_layout()\n","    plt.show()"]}],"metadata":{"accelerator":"TPU","colab":{"authorship_tag":"ABX9TyONJYwwrREOBnYeGiWm81WS","machine_shape":"hm","mount_file_id":"1QC1bcZ8VIFIvaGjBfKrSwE6arKvKz6tV","name":"","provenance":[{"file_id":"1QC1bcZ8VIFIvaGjBfKrSwE6arKvKz6tV","timestamp":1696236228813}],"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}