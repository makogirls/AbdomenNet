{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["Oi899-kZ46Js"],"authorship_tag":"ABX9TyPZZ+ZFpvuF/GovTSBPsOHt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Baseline"],"metadata":{"id":"XoDYXhO_vPp2"}},{"cell_type":"markdown","source":["Input ▶ Data Preprocessing ▶ Training ▶ Model Evaluation ▶ Load the model ▶ Preprocess the data ▶ Inference"],"metadata":{"id":"LM5rnKoSwv4f"}},{"cell_type":"markdown","source":["## Input"],"metadata":{"id":"n16-bv3u3qYQ"}},{"cell_type":"markdown","source":["**Dataset AbdomenCT-1K**<br>\n","[Ma et al., 2021](https://arxiv.org/abs/2010.14808#:~:text=AbdomenCT%2D1K%3A%20Is%20Abdominal%20Organ%20Segmentation%20A%20Solved%20Problem%3F,-Jun%20Ma%2C%20Yao&text=With%20the%20unprecedented%20developments%20in,variability%20on%20many%20benchmark%20datasets.) - [AbodmenCT-1K](https://github.com/JunMa11/AbdomenCT-1K)"],"metadata":{"id":"dgSLjghf7Zwb"}},{"cell_type":"markdown","source":["**INPUT.shape = (N, C, H, W, D)**<br>\n","N is batch size, C is channel, H is height, W is width, and D is depth of the INPUT.shape<br>\n","\n","e.g. example:\n","- (N, 1, 225, 225, 1)\n","- (N, 1, 225, 225, 10)\n","- (N, 1, 225, 225, 15)"],"metadata":{"id":"_Ko5QdxpxAL_"}},{"cell_type":"markdown","source":["---"],"metadata":{"id":"vK19-Yse5CFB"}},{"cell_type":"markdown","source":["## Data Preprocessing"],"metadata":{"id":"HG40aMsT3r5A"}},{"cell_type":"markdown","source":["**Data Preprocessing**<br>\n","- CODE - TBA\n","- [Kaggle - inference](https://www.kaggle.com/code/aritrag/kerascv-starter-notebook-infer)\n","- [MIDL2021-CT-Classification](https://github.com/vinbigdata-medical/MIDL2021-CT-Classification) - [Preprocess DICOM image](https://github.com/vinbigdata-medical/MIDL2021-CT-Classification#preprocess-dicom-image)"],"metadata":{"id":"92597A2-3uDD"}},{"cell_type":"markdown","source":["---"],"metadata":{"id":"dRpBYaMW5AwD"}},{"cell_type":"markdown","source":["## Training"],"metadata":{"id":"B_mhulnX3yeL"}},{"cell_type":"markdown","source":["What is the task?\n","- Multilabel Multiclass Classification\n","- Ensemble (Binary, Multiclass) Classification\n","- Segmentation -> Classification\n","\n","What is the dimension of the inputs?\n","- 2D, 2.5D, 3D\n","\n","Which framework should we use?\n","- Keras\n","- PyTorch\n","- Tensorflow\n","\n","Which data augmentation should we use?\n","- Augmentation\n","\n","Which model should we use?\n","- ResNet, EfficientNet, MobileNet with pretrained weights\n","\n"],"metadata":{"id":"RZTZeOfZ3z-Z"}},{"cell_type":"markdown","source":["Model Review\n","---\n","Task / time / input / performance / reference / model why / limitation\n","--\n","|Model|Task|time|performance|reference|why|limitation|efficient framework|.   |\n","|------|---|---|---|---|---|---|---|---|\n","|EfficientNet 시리즈|classification|fast|모델 크기에 따라 다름|O|RNSA에서 많이 씀|데이터 셋 크기 주의|pytorch/tensorflow|EfficientNet-B6? EfficientNet-B5? EfficientNetV2?\n","|VGG 시리즈|classification/object detection|slow|대규모 효과적|O|multiclass 많이 씀|오래걸림|pytorch/tensorflow|\n","|ResNet 시리즈|classification/object detection/semantic segmentation|normal|good|O|RSNA에서 많이 씀|계층 주의|pytorch/tensorflow|ResNet101, ResNet152|\n","|Unet|segmentation|normal|good|O|의료분야 많이씀|데이터셋, 라벨|pytorch/tensorflow|다른 구조의 백본이랑 같이..\n","|GoogLeNet(Inception)|classification/object detection|fast|가볍다|O|의료분야 씀|복잡, 오버피팅 주의|tensorflow|\n","|Xception|classficiation/object detection|can be slow|good|O|multiclass에 사용됨|깊은 구조|tensorflow/keras\n","|DenseNet|classficiation/object detection|normal|good|O|RNSA/파라미터 공유|시간 주의|pytorch\n","|FusionNet|3D image object classification|마다 다름|multi modal|O|keras참고|데이터 표준화 주의|all"],"metadata":{"id":"4MyiTSmN9f8G"}},{"cell_type":"markdown","source":["### 1. Kaggle"],"metadata":{"id":"TSTPQ-bO8ymu"}},{"cell_type":"markdown","source":["ML Model - [Abdominal Trauma Detection - ResNet-152 - Pytorch](https://www.kaggle.com/code/witoldnowogrski/abdominal-trauma-detection-resnet-152-pytorch)<br>\n","Tabular data -"],"metadata":{"id":"4MAsoaREzQI_"}},{"cell_type":"markdown","source":["### 2. Competition - RSNA"],"metadata":{"id":"iQKVRosz803v"}},{"cell_type":"markdown","source":["[Spine Fracture Detection](https://www.kaggle.com/competitions/rsna-2022-cervical-spine-fracture-detection/discussion/365115)\n","\n","**1st**\n","- 3D segmentation : Resnet-18 + Unet +EffNetV2-Small backbone\n","- 2.5D classification : EffNetV2-Small, ConvNext Tiny, ConvNext Nano, NFNet-L0\n","\n","**2nd** 2.5D, 3channel\n","\n","- stage1： 2.5D CNN + Unet for Segmentation (efficientnet-b0 backbone)\n","- stage2： CNN + BiGRU + Attention mechanism for Classification\n","\n","**3rd**\n","- Architecture : 2.5D CNN + 1D RNN\n","- backbone : resnest50d + seresnext50\n","\n","**4th**\n","- 3D segmentation : Unet + ir-CSN-50\n","- 3D classification : ir(ip)-CSN-152\n","\n","**5th**\n","- EfficientNet-B5 (Classification)\n","- EfficientNet-B3-UNet (Segmentation)\n","\n","[Mammography Breast Cancer Detection](https://www.kaggle.com/competitions/rsna-breast-cancer-detection/discussion/392449)\n","- 4 x Convnextv1-small 2048x1024, validated on 4-folds splits\n","- YOLOX\n","\n","[Covid19 Detection](https://www.kaggle.com/competitions/siim-covid19-detection/discussion/263658)\n","\n","**1st** Multi-task classification + segmentation\n","\n","- Ensemble of 4 models (Yolov5X6 input size 768 + EfficientDet D7 input size 768 + FasterRNN FPN resnet101 input size 1024 + FasterRNN FPN resnet200 input size 768) using weighted boxes fusion (IoU 0.6)\n","\n","**3rd** hybrid classification-segmentation model\n","\n","- Classification : architecture used EfficientNet-B6 and DeepLabV3+ while the other used Swin Transformer and FPN\n","- Segmentation : EfficientDet + MMDetection (ffDet-D7X, EffDet-D6, Swin-RepPoints)\n","\n","[VinBigData Chest X-ray Abnormalities Detection](https://)\n","\n","**1st** : Ensembling Detectron2 + YOLOv5\n","\n","**2nd** : Ensembling YOLOv5, ResNet101, ResNet152, and HourGlass"],"metadata":{"id":"5aNDW4Eo83lU"}},{"cell_type":"markdown","source":["### 3. We can adjust the backbone with other models"],"metadata":{"id":"bEsbhOk-40H0"}},{"cell_type":"markdown","source":["```python\n","def build_model(warmup_steps, decay_steps):\n","    # Define Input\n","    inputs = keras.Input(shape=config.IMAGE_SIZE + [3,], batch_size=config.BATCH_SIZE)\n","    \n","    # Define Backbone\n","    backbone = keras_cv.models.ResNetBackbone.from_preset(\"resnet50_imagenet\")\n","    backbone.include_rescaling = False\n","    x = backbone(inputs)\n","    \n","    # GAP to get the activation maps\n","    gap = keras.layers.GlobalAveragePooling2D()\n","    x = gap(x)\n","\n","    # Define 'necks' for each head\n","    x_bowel = keras.layers.Dense(32, activation='silu')(x)\n","    x_extra = keras.layers.Dense(32, activation='silu')(x)\n","    x_liver = keras.layers.Dense(32, activation='silu')(x)\n","    x_kidney = keras.layers.Dense(32, activation='silu')(x)\n","    x_spleen = keras.layers.Dense(32, activation='silu')(x)\n","\n","    # Define heads\n","    out_bowel = keras.layers.Dense(1, name='bowel', activation='sigmoid')(x_bowel) # use sigmoid to convert predictions to [0-1]\n","    out_extra = keras.layers.Dense(1, name='extra', activation='sigmoid')(x_extra) # use sigmoid to convert predictions to [0-1]\n","    out_liver = keras.layers.Dense(3, name='liver', activation='softmax')(x_liver) # use softmax for the liver head\n","    out_kidney = keras.layers.Dense(3, name='kidney', activation='softmax')(x_kidney) # use softmax for the kidney head\n","    out_spleen = keras.layers.Dense(3, name='spleen', activation='softmax')(x_spleen) # use softmax for the spleen head\n","    \n","    # Concatenate the outputs\n","    outputs = [out_bowel, out_extra, out_liver, out_kidney, out_spleen]\n","\n","    # Create model\n","    print(\"[INFO] Building the model...\")\n","    model = keras.Model(inputs=inputs, outputs=outputs)\n","    \n","    # Cosine Decay\n","    cosine_decay = keras.optimizers.schedules.CosineDecay(\n","        initial_learning_rate=1e-4,\n","        decay_steps=decay_steps,\n","        alpha=0.0,\n","        warmup_target=1e-3,\n","        warmup_steps=warmup_steps,\n","    )\n","\n","    # Compile the model\n","    optimizer = keras.optimizers.Adam(learning_rate=cosine_decay)\n","    loss = {\n","        \"bowel\":keras.losses.BinaryCrossentropy(),\n","        \"extra\":keras.losses.BinaryCrossentropy(),\n","        \"liver\":keras.losses.CategoricalCrossentropy(),\n","        \"kidney\":keras.losses.CategoricalCrossentropy(),\n","        \"spleen\":keras.losses.CategoricalCrossentropy(),\n","    }\n","    metrics = {\n","        \"bowel\":[\"accuracy\"],\n","        \"extra\":[\"accuracy\"],\n","        \"liver\":[\"accuracy\"],\n","        \"kidney\":[\"accuracy\"],\n","        \"spleen\":[\"accuracy\"],\n","    }\n","    print(\"[INFO] Compiling the model...\")\n","    model.compile(\n","        optimizer=optimizer,\n","      loss=loss,\n","      metrics=metrics\n","    )\n","    \n","    return model\n","```"],"metadata":{"id":"BME-rAuRzLxB"}},{"cell_type":"markdown","source":["### 4. Literature Review"],"metadata":{"id":"vFh1SOBU5ndF"}},{"cell_type":"markdown","source":["1. [MIDL2021-CT-Classification](https://github.com/vinbigdata-medical/MIDL2021-CT-Classification)\n","2. [GP LEE et al. 2023](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10241572/#:~:text=PMID%3A%2037284168-,Enhancing%20Disease%20Classification%20in%20Abdominal%20CT%20Scans%20through%20RGB%20Superposition,Study%20of%20Appendicitis%20and%20Diverticulitis) - [PDF](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10241572/pdf/CMMM2023-7714483.pdf)\n","3. [Liu et al., 2023](https://github.com/ljwztc/CLIP-Driven-Universal-Model) - [arXiv](https://arxiv.org/abs/2301.00785)\n","4. [Ma et al., 2021](https://arxiv.org/abs/2010.14808#:~:text=AbdomenCT%2D1K%3A%20Is%20Abdominal%20Organ%20Segmentation%20A%20Solved%20Problem%3F,-Jun%20Ma%2C%20Yao&text=With%20the%20unprecedented%20developments%20in,variability%20on%20many%20benchmark%20datasets.) - [AbodmenCT-1K](https://github.com/JunMa11/AbdomenCT-1K)"],"metadata":{"id":"iyJDnc_j5540"}},{"cell_type":"markdown","source":["---"],"metadata":{"id":"3C0y-SpG5QIM"}},{"cell_type":"markdown","source":["## Model Evaluation"],"metadata":{"id":"iuhXhVoM4dM_"}},{"cell_type":"markdown","source":["How to compare the model performance?\n","- GroupKFold (K = 5) only for performance check\n","- patientID as group, elements are seriesID\n","\n","\n","\n","\n","\n"],"metadata":{"id":"5SL-qmgFvwul"}},{"cell_type":"markdown","source":["---"],"metadata":{"id":"qHYcHZQT5RZZ"}},{"cell_type":"markdown","source":["## Load the model"],"metadata":{"id":"rNfECxM44If0"}},{"cell_type":"markdown","source":["Follow the training infer reference:<br>\n","- [Train](https://www.kaggle.com/code/aritrag/kerascv-starter-notebook-train)\n","- [Infer](https://www.kaggle.com/code/jihoonkim2100/kerascv-starter-notebook-infer)\n"],"metadata":{"id":"CehLI4X24LYT"}},{"cell_type":"markdown","source":["---"],"metadata":{"id":"nD7F1Ry05Std"}},{"cell_type":"markdown","source":["## Preprocess the data"],"metadata":{"id":"IksvGLx34Xwn"}},{"cell_type":"markdown","source":["**Data Preprocessing**<br>\n","- CODE - TBA\n","- [Reference](https://www.kaggle.com/code/aritrag/kerascv-starter-notebook-infer)"],"metadata":{"id":"N6P1ouEa4YAv"}},{"cell_type":"markdown","source":["---"],"metadata":{"id":"MaV_Jvkr5UFR"}},{"cell_type":"markdown","source":["## Inference"],"metadata":{"id":"Oi899-kZ46Js"}},{"cell_type":"markdown","source":["Inference"],"metadata":{"id":"ZH65JT_O4859"}},{"cell_type":"markdown","source":["---"],"metadata":{"id":"gm2c1M_R5VoV"}},{"cell_type":"code","source":[],"metadata":{"id":"1K6xIjWK5V8V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Score"],"metadata":{"id":"dGO8l4puTFlw"}},{"cell_type":"code","source":[],"metadata":{"id":"2SS2Qqn6TRYc"},"execution_count":null,"outputs":[]}]}